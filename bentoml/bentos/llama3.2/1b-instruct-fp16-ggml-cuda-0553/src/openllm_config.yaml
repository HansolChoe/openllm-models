engine_config:
  filename: Llama-3.2-1B-Instruct-F16.gguf
  max_model_len: 2048
  model: unsloth/Llama-3.2-1B-Instruct-GGUF
extra_envs:
- name: CMAKE_ARGS
  value: -DGGML_CUDA=on
- name: HF_TOKEN
extra_labels:
  model_name: unsloth/Llama-3.2-1B-Instruct-GGUF
  openllm_alias: llama3.2,1b-instruct-fp16-ggml-cuda
  platforms: linux
project: llamacpp-chat
service_config:
  name: llama3.2
  resources:
    gpu: 1
    gpu_type: nvidia-rtx-3060
    memory: 3Gi
  traffic:
    timeout: 300

envs:
- name: CMAKE_ARGS
  value: -DGGML_CUDA=on
- name: HF_TOKEN
include:
- '*.py'
- '*.yaml'
- ui/*
labels:
  model_name: unsloth/Llama-3.2-1B-Instruct-GGUF
  openllm_alias: llama3.2,1b-instruct-fp16-ggml
  platforms: linux,macos
  source: https://github.com/bentoml/openllm-models-feed/tree/main/src/llamacpp-chat
python:
  lock_packages: true
  requirements_txt: ./requirements.txt
service: service:LlamaCppChat
